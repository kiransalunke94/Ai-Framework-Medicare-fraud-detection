{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe456dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from pandas import read_csv\n",
    "from kafka.admin import NewTopic,KafkaAdminClient\n",
    "from kafka import KafkaConsumer\n",
    "import time\n",
    "from os import getcwd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,from_json,col\n",
    "from pyspark.sql.types import StringType,StructType,IntegerType,DoubleType,ArrayType,StructField,TimestampType,DateType\n",
    "from pyspark.sql.functions import format_number, dayofmonth, hour, dayofyear,month, year, weekofyear, date_format,to_date\n",
    "#from consumer import consumer_task # user-defined module\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0711ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Weather_StrcturedStreaming\") \\\n",
    "  .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "  .config(\"spark.streaming.receiver.writeAheadLog.enable\", \"true\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b4de9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32b9341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kafka Streams\n",
    "BOOTSTRAP_SERVERS = 'localhost:9092' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4adea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", 'localhost:9092') \\\n",
    "    .option(\"subscribe\", 'Weather_data') \\\n",
    "    .option(\"startingOffsets\", 'earliest') \\\n",
    "    .option(\"failOnDataLoss\",\"true\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc941af",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType().add('CityName', StringType(), False)\\\n",
    "    .add('Temperature', DoubleType(), False)\\\n",
    "    .add('Humidity', IntegerType(), False)\\\n",
    "    .add('Weather', StringType(), False)\\\n",
    "    .add('Time', TimestampType(), False)\n",
    "\n",
    "df_1 = df.selectExpr('CAST(value AS STRING)').select(from_json('value', schema).alias('temp')).select('temp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1 = df_1.withColumn('Date',to_date(df_1['Time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_1.writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .option(\"checkpointLocation\", getcwd()+\"/checkpoint_dir\") \\\n",
    "  .format(\"console\") \\\n",
    "  .trigger(processingTime= \"1 seconds\") \\\n",
    "  .queryName(\"Weather_1producer\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdcd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.writeStream \\\n",
    "    .format(source = \"csv\") \\\n",
    "    .option(\"path\",\"hdfs://localhost:9000/kiran/weather_data\") \\\n",
    "    .option(\"checkpointlocation\",\"hdfs://localhost:9000/kiran/weather_data_checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c48a86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df3.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d5102",
   "metadata": {},
   "source": [
    "#def json_deserializer(data):\n",
    "    #return json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba2d70",
   "metadata": {},
   "source": [
    "#consumer = KafkaConsumer(\"Weather_data\",bootstrap_servers=\n",
    "\n",
    "#['localhost:9092'],auto_offset_reset='latest',group_id=\"group-1\",value_deserializer=json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca5d8c",
   "metadata": {},
   "source": [
    "### for message in consumer:\n",
    "    #print(message.value)\n",
    "    \n",
    "    #Converting string type to dict\n",
    "    mess = json.dumps(message.value)\n",
    "    \n",
    "    print(mess)\n",
    "    #Accessing dict with key\n",
    "    #if mess[\"temp\"] > 25:\n",
    "        #print(\"Temperature high\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
